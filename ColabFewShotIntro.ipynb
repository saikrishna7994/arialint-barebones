{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ColabFewShotIntro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saikrishna7994/arialint-barebones/blob/master/ColabFewShotIntro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffF-KvUFg9bX"
      },
      "source": [
        "## Intro\n",
        "\n",
        "Hi, this notebook's going to show you the basics of how to use colab for GPT development.\n",
        "\n",
        "Basically, we need to do 2 things:\n",
        "- Upload our credentials\n",
        "- Send prompts to the OpenAI Servers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPCBXBgJk9Oq"
      },
      "source": [
        "We'll start by installing the relevant packages we need, we can talk about them later. To run a cell containing python code (for example, the one below that starts \"!pip install openai\"), select it and hit shift+enter or click the little \"play\" arrow in a circle at the upper left of the cell. You should run through each cell in order, since the cells depend on variables from previous cells, and some of those variables, like \"prompt\", get redefined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osJazaYhlBul",
        "outputId": "4a0f9bb7-15dd-4a07-a836-8de3c24f83d7"
      },
      "source": [
        "!pip install openai\n",
        "import openai, json, pandas as pd\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/91/070425a1983c1a73b9a6186b28ac06ae87e619fff05257e4ae9b8078f26f/openai-0.3.0.tar.gz (157kB)\n",
            "\r\u001b[K     |██                              | 10kB 15.1MB/s eta 0:00:01\r\u001b[K     |████▏                           | 20kB 16.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 30kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 40kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 51kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 61kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 71kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 81kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 92kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 102kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 112kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 122kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 133kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 143kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 153kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 163kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.6/dist-packages (from openai) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->openai) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->openai) (3.0.4)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.3.0-cp36-none-any.whl size=171252 sha256=d2b450d9b568e7e2a0b921bec21a24ce6870dc723d551a2922edc74aadeaa58a\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/75/b3/519250ae7ef404a76542edfbb7a290c78ebb314c5b8924f541\n",
            "Successfully built openai\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZTogccBhSvh"
      },
      "source": [
        "We'll upload our API key. I like loading up a .json file with the format:\n",
        "\n",
        "{'key': 'API KEY GOES HERE'}\n",
        "\n",
        "You can also just use\n",
        "\n",
        "openai.api_key = 'KEY'\n",
        "\n",
        "and replace the word 'KEY' with the long alphanumeric key starting with \"sk-\" you can find here: https://beta.openai.com/docs/api-reference/authentication but I do this here so we can share this without exposing keys by accident. After using shift+enter on the cell below, it will prompt you to select the key.json file from your directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "6Gmjnn4zjPUI",
        "outputId": "98e0284b-9d32-45a4-dc97-bcab8de133e3"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "print(\"done\")\n",
        "openai.api_key = json.load(open(\"key.json\", \"r\"))[\"key\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3c0d26f7-d962-475a-adad-902d31bc7c9c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3c0d26f7-d962-475a-adad-902d31bc7c9c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving key.json to key.json\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oETSqUnplzvE"
      },
      "source": [
        "## Making our first prompt\n",
        "\n",
        "Making a prompt in colab is easy. We just write a string and send it to OpenAI to autocomplete.\n",
        "\n",
        "Let's make our first string."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKlDK3IOQ32b"
      },
      "source": [
        "Simple Translation: English to French:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaGDtW-ilQRf"
      },
      "source": [
        "prompt = \"\"\"We translated each sentence from English to French.\n",
        "\n",
        "English: I want to make some tacos\n",
        "French:\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUJW4BqcRK7N",
        "outputId": "e5b51a29-22f2-4ff1-a2b6-e6677e2c3735"
      },
      "source": [
        "print(openai.Completion.create(prompt=prompt, engine='davinci', stop='\\n', max_tokens=20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"text\": \" Je veux faire quelque tacos\"\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1612801502,\n",
            "  \"id\": \"cmpl-2R3dOsGI1eBBZlieLFvMoTyrTSw23\",\n",
            "  \"model\": \"davinci:2020-05-03\",\n",
            "  \"object\": \"text_completion\"\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta_Bm7AaRjJ0"
      },
      "source": [
        "Great, we can see that it returned choices and stuff. We'll covering requesting multiple completions later, for now, let's just grab the completion['choices'][0]['text'] from now on. That's GPT-3's best guess at what should come next after the text we gave it in the prompt.\n",
        "\n",
        "Now we can also give it more examples and see if it can translate into other languages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRkrIl1WRYYC"
      },
      "source": [
        "prompt = \"\"\"We translated each sentence from English to other languages.\n",
        "\n",
        "English: I want to make some tacos\n",
        "French: Je veux faire des tacos\n",
        "\n",
        "English: Where is the library?\n",
        "Spanish:\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UNY8rIVjR5ae",
        "outputId": "eb9602f8-cb76-47cc-b165-7d1e63f0b1e9"
      },
      "source": [
        "openai.Completion.create(prompt=prompt, engine='davinci', stop='\\n', max_tokens=20)['choices'][0]['text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' ¿Dónde está la biblioteca?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFjxkTqaR_cm"
      },
      "source": [
        "That was pretty cool! I bet we can even do other stuff like fix spelling and grammar:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwUWpB3zR9Wg"
      },
      "source": [
        "prompt = \"\"\"We fixed the spelling and grammar of each sentence.\n",
        "\n",
        "Original: I wants some tacos\n",
        "Correct: I want some tacos\n",
        "\n",
        "Original: They is really nice.\n",
        "Correct:\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Me-WHH0NSRUP",
        "outputId": "6e4fccf4-43cb-438d-8fa6-80017f5a2872"
      },
      "source": [
        "openai.Completion.create(prompt=prompt, engine='davinci', stop='\\n', max_tokens=20)['choices'][0]['text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' They are really nice.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48ihOSlKSTb8"
      },
      "source": [
        "Or change politeness while keeping roughly the same meaning:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrtkLAaCSR2F"
      },
      "source": [
        "prompt = \"\"\"We made everything polite.\n",
        "\n",
        "Original: You suck\n",
        "Translation: I don't think you did that well.\n",
        "\n",
        "Original: Shut up.\n",
        "Translation:\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8JxAISFZSk5Y",
        "outputId": "52d532ba-8424-4a7b-bb1f-f6a9eb9251bb"
      },
      "source": [
        "openai.Completion.create(prompt=prompt, engine='davinci', stop='\\n', max_tokens=20)['choices'][0]['text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Look, it's really not my fault.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClfPDcIbS07L"
      },
      "source": [
        "Huh, so that didn't work with only 2 examples. If you hit shift-enter more than once on the same cell, you can see other possible results. Let's try giving it some more examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOXhm9AdSlhN"
      },
      "source": [
        "prompt = \"\"\"We made everything polite.\n",
        "\n",
        "Original: You suck\n",
        "Polite: I don't think you did that well.\n",
        "\n",
        "Original: Shut up.\n",
        "Polite: Please be quiet.\n",
        "\n",
        "Original: Give me that.\n",
        "Polite:\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IhktKN44S9yD",
        "outputId": "ae95ee14-ffe8-4672-b3de-1fba2ea8064e"
      },
      "source": [
        "openai.Completion.create(prompt=prompt, engine='davinci', stop='\\n', max_tokens=20)['choices'][0]['text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' I have nothing more to say to you.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH8A3Ki-L3-6"
      },
      "source": [
        "Let's check how we're doing so far: here's a challenge, how would we get a prompt to generate a translation from English to German?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdLVBwC7MAjP"
      },
      "source": [
        "prompt = \"\"\"YOUR CODE HERE\n",
        "\n",
        "English: We'll cross that bridge when we come to it\n",
        "German:\"\"\"\n",
        "\n",
        "openai.Completion.create(prompt=prompt, engine='davinci', stop='\\n', max_tokens=20)['choices'][0]['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bICma6sTL7a"
      },
      "source": [
        "Try adding more and more examples, rerunning both the prompt cell and then the completion cell, until it behaves the way you want. We can do style transfer as well"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2ESex8QS-Yq"
      },
      "source": [
        "prompt = \"\"\"Author: Edgar Allen Poe\n",
        "Topic: A Raven\n",
        "Poem: But the Raven, sitting lonely on the placid bust, spoke only\n",
        "That one word, as if his soul in that one word he did outpour.\n",
        "Nothing further then he uttered—not a feather then he fluttered—\n",
        "Till I scarcely more than muttered \"Other friends have flown before—\n",
        "On the morrow he will leave me, as my hopes have flown before.\"\n",
        "Then the bird said \"Nevermore.\"\n",
        "\n",
        "Author: Walt Whitman\n",
        "Topic: A Captain\n",
        "Poem: O Captain! my Captain! our fearful trip is done,\n",
        "The ship has weather’d every rack, the prize we sought is won,\n",
        "The port is near, the bells I hear, the people all exulting,\n",
        "While follow eyes the steady keel, the vessel grim and daring;\n",
        "\n",
        "Author: Robert Frost\n",
        "Topic: Walking in the woods\n",
        "Poem: Whose woods these are I think I know.   \n",
        "His house is in the village though;   \n",
        "He will not see me stopping here   \n",
        "To watch his woods fill up with snow.   \n",
        "\n",
        "Author: Edgar Allen Poe\n",
        "Topic: Why pineapples on pizza are bad\n",
        "Poem:\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Vb4za30Tfre",
        "outputId": "c00f106e-d39d-434c-9d07-b8d3eabf702e"
      },
      "source": [
        "r = openai.Completion.create(prompt=prompt, engine='davinci', stop=['\\n\\n', 'Poem'], max_tokens=40)['choices'][0]['text']\n",
        "print(r)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " But, most from that most evil pair, \n",
            "Who taught thee, monster, to devour \n",
            "The mangiest fragments of the earth \n",
            "And gods? Descend, ye shades,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0WAmc3UT2qg"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBePfesiTgRx"
      },
      "source": [
        "prompt = \"\"\"Author: Edgar Allen Poe\n",
        "Topic: A Raven\n",
        "Poem: But the Raven, sitting lonely on the placid bust, spoke only\n",
        "That one word, as if his soul in that one word he did outpour.\n",
        "Nothing further then he uttered—not a feather then he fluttered—\n",
        "Till I scarcely more than muttered \"Other friends have flown before—\n",
        "On the morrow he will leave me, as my hopes have flown before.\"\n",
        "Then the bird said \"Nevermore.\"\n",
        "\n",
        "Author: Walt Whitman\n",
        "Topic: A Captain\n",
        "Poem: O Captain! my Captain! our fearful trip is done,\n",
        "The ship has weather’d every rack, the prize we sought is won,\n",
        "The port is near, the bells I hear, the people all exulting,\n",
        "While follow eyes the steady keel, the vessel grim and daring;\n",
        "\n",
        "Author: Robert Frost\n",
        "Topic: Walking in the woods\n",
        "Poem: Whose woods these are I think I know.   \n",
        "His house is in the village though;   \n",
        "He will not see me stopping here   \n",
        "To watch his woods fill up with snow.   \n",
        "\n",
        "Author: Robert Frost\n",
        "Topic: Why pineapples on pizza are bad\n",
        "Poem:\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c73kf3q7VygO",
        "outputId": "e4ad65c0-5364-4a14-810a-61bfd6df1f61"
      },
      "source": [
        "r = openai.Completion.create(prompt=prompt, engine='davinci', stop=['\\n\\n', 'Poem'], max_tokens=40)['choices'][0]['text']\n",
        "print(r)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Why pineapples on pizza?\n",
            "Don’t go there.\n",
            "Ask Them why the ruler’s straight.\n",
            "Don’t ask Them why the swallows fly. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWQnjBl9V54I"
      },
      "source": [
        "# Getting it to process things a bit more\n",
        "\n",
        "So it's kind of mindlessly auto-completing so far. We can give it some ideas of what to do in non-normal circumstances:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2DlKXWjWBsu"
      },
      "source": [
        "Nonsense Detections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hYYdqPxVzcL"
      },
      "source": [
        "prompt = \"\"\"Q: What is 1+1?\n",
        "A: 2\n",
        "\n",
        "Q: What is the capital of France?\n",
        "A: Paris\n",
        "\n",
        "Q: What is the fooza of Spain?\n",
        "A:\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUi9VC1qWLVs",
        "outputId": "20e783d4-dd9d-4528-d780-e9b9c4b7640a"
      },
      "source": [
        "r = openai.Completion.create(prompt=prompt, engine='davinci', stop=['\\n\\n'], max_tokens=40)['choices'][0]['text']\n",
        "print(r)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Madrid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0wXVvpUWOST"
      },
      "source": [
        "It got the first and second questions right. How did it \"know\"?\r\n",
        "\r\n",
        "There were enough examples of people asking the capital of France, or people telling the capital of France, in the material GPT-3 was trained on that \"Paris\" was the highest probabity word to occur next in that context. This happens for many, many different concepts. GPT-3 contains, in the implicit strengths of its neural network weights, many facts about the world and how they are related to each other, and can use those to answer questions or complete prompts in ways that make sense.\r\n",
        "\r\n",
        "On the third question, though, this doesn't work. It has never seen the word \"fooza\" before. Sometimes it makes up a word. Sometimes it gives the **capital** of Spain, which isn't what we want, either. GPT-3 gets that the context is always supplying the correct answer, but since fooza isn't a word, what it comes up with will be some kind of nonsense. This is an important point: although often GPT-3 tells true facts that it has picked up from its training, it can also just make up an answer-- we call that \"confabulating.\" GPT-3 isn't like a database, where you can rely on it always producing the same true answer that it was given in the first place. It's more like an improv actor, who has to act like they know the answer even when they don't, so just tries to fake it.\r\n",
        "\r\n",
        "Let's give it examples of what to do if the input is nonsensical:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwAoSvOzWM4V"
      },
      "source": [
        "prompt = \"\"\"Q: What is 1+1?\n",
        "A: 2\n",
        "\n",
        "Q: What's a falala duobla?\n",
        "A: That doesn't make sense.\n",
        "\n",
        "Q: What is the capital of France?\n",
        "A: Paris\n",
        "\n",
        "Q: How does a famghorn eat cheese?\n",
        "A: That doesn't make sense.\n",
        "\n",
        "Q: What is the fooza of Spain?\n",
        "A:\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKl49Fq0WXOl",
        "outputId": "750ea357-6b64-4e23-ce3f-a26cd654d900"
      },
      "source": [
        "r = openai.Completion.create(prompt=prompt, engine='davinci', stop=['\\n\\n'], max_tokens=40)['choices'][0]['text']\n",
        "print(r)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " That doesn't make sense.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9lsCleFWXyn"
      },
      "source": [
        "prompt = \"\"\"Q: What is 1+1?\n",
        "A: 2\n",
        "\n",
        "Q: What's a falala duobla?\n",
        "A: That doesn't make sense.\n",
        "\n",
        "Q: What is the capital of France?\n",
        "A: Paris\n",
        "\n",
        "Q: How does a famghorn eat cheese?\n",
        "A: That doesn't make sense.\n",
        "\n",
        "Q: Who was the first US president?\n",
        "A:\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SowKOf4VWkuR",
        "outputId": "c34ba88a-6f5a-4cc8-90e1-a26db913ef54"
      },
      "source": [
        "r = openai.Completion.create(prompt=prompt, engine='davinci', stop=['\\n\\n'], max_tokens=40)['choices'][0]['text']\n",
        "print(r)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " George Washington\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUgTptehWk94"
      },
      "source": [
        "prompt = \"\"\"Q: What is 1+1?\n",
        "A: 2\n",
        "\n",
        "Q: What's a falala duobla?\n",
        "A: That doesn't make sense.\n",
        "\n",
        "Q: What is the capital of France?\n",
        "A: Paris\n",
        "\n",
        "Q: How does a famghorn eat cheese?\n",
        "A: That doesn't make sense.\n",
        "\n",
        "Q: Who was the first US king?\n",
        "A:\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOupkuaGW1bK",
        "outputId": "70078d0a-ae91-4a8d-e1b0-95438bc74af3"
      },
      "source": [
        "r = openai.Completion.create(prompt=prompt, engine='davinci', stop=['\\n\\n'], max_tokens=40)['choices'][0]['text']\n",
        "print(r)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " George Washinton\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkP3JhdVW3x3"
      },
      "source": [
        "OK, those initial ones don't help it get quite there, but we can see it's doing better. What examples could we add to the prompt so that it would correctly answer that the US doesn't have a king?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42vpRBooMOPv"
      },
      "source": [
        "prompt = \"\"\"YOUR PROMPT HERE\n",
        "\n",
        "Q: Who was the first US king?\n",
        "A:\"\"\"\n",
        "\n",
        "r = openai.Completion.create(prompt=prompt, engine='davinci', stop=['\\n\\n'], max_tokens=40)['choices'][0]['text']\n",
        "print(r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3rT0M5-Zhzb"
      },
      "source": [
        "# Part 3: Classification\n",
        "\n",
        "We can give GPT examples for classifying by labels both implicit and explicit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxzN9BUhW17M"
      },
      "source": [
        "prompt = \"\"\"We labelled each sentence with whether it's about cats, dogs, or neither\n",
        "\n",
        "Sentence: I like canines\n",
        "About:\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tywb7qXMZyA0",
        "outputId": "083530bb-de58-4ef5-eb9c-74c4594c893a"
      },
      "source": [
        "r = openai.Completion.create(prompt=prompt, engine='davinci', stop=['\\n'], max_tokens=40)['choices'][0]['text']\n",
        "print(r)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Dogs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyXBpWh5Zypq"
      },
      "source": [
        "prompt = \"\"\"We labelled each sentence with whether it's about cats, dogs, or neither\n",
        "\n",
        "Sentence: I like felines\n",
        "About:\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyLJ1lNHZ1um",
        "outputId": "9f1edda1-0706-4ede-aa2c-b3453df04e5e"
      },
      "source": [
        "r = openai.Completion.create(prompt=prompt, engine='davinci', stop=['\\n'], max_tokens=40)['choices'][0]['text']\n",
        "print(r)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Cats\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNBgYwaiZ2NO",
        "outputId": "b2e153bb-4c10-4079-8df4-57464a374043"
      },
      "source": [
        "prompt = \"\"\"We labelled each sentence with whether it's about cats, dogs, or neither\n",
        "\n",
        "Sentence: I like turtles\n",
        "About:\"\"\"\n",
        "\n",
        "r = openai.Completion.create(prompt=prompt, engine='davinci', stop=['\\n'], max_tokens=40)['choices'][0]['text']\n",
        "print(r)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " none\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kikySEYZZ5c4"
      },
      "source": [
        "Huh, so apparently that works sometimes but don't assume it always will. More examples can help..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwNoBYoWZ4k0",
        "outputId": "71656b35-9dad-4ef4-e7d3-fb318bfaaf95"
      },
      "source": [
        "prompt = \"\"\"We gave each synopsis the appropriate TV rating\n",
        "\n",
        "Synopsis: A dog and a cat become friends.\n",
        "Rating: PG\n",
        "\n",
        "Synopsis: Two people have a lot of sex\n",
        "Rating:\"\"\"\n",
        "\n",
        "r = openai.Completion.create(prompt=prompt, engine='davinci', stop=['\\n'], max_tokens=40)['choices'][0]['text']\n",
        "print(r)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " R\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5I0J390aQOq"
      },
      "source": [
        "# Taking a look at the parameters we haven't been messing with"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO7szwQzxurD"
      },
      "source": [
        "K, let's take another look at what's going on under the hood here. We'll start by going and requesting all the data we can actually get back from a completion.\n",
        "\n",
        "Let's try... math."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oad4O_h6aMbr"
      },
      "source": [
        "prompt = \"\"\"1+1=2\n",
        "2+4=\"\"\"\n",
        "r = openai.Completion.create(prompt=prompt, engine='davinci', stop=['\\n'], max_tokens=40)['choices'][0]['text']\n",
        "print(r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIzgV7YmyWKn"
      },
      "source": [
        "let's request some more data by getting the logprobs. The logprobs tell us the probabilities of each token that'll come next. We don't want the text anymore when we get this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YJ_ZCgFx9ZX",
        "outputId": "62afe901-12aa-42b8-ac4c-4c89d3a1f5b0"
      },
      "source": [
        "r = openai.Completion.create(prompt=prompt, engine='davinci', stop=['\\n'], max_tokens=40, logprobs=5)\n",
        "print(r['choices'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"finish_reason\": \"stop\",\n",
            "  \"index\": 0,\n",
            "  \"logprobs\": {\n",
            "    \"text_offset\": [\n",
            "      10,\n",
            "      11,\n",
            "      11,\n",
            "      11,\n",
            "      11,\n",
            "      11,\n",
            "      11,\n",
            "      11\n",
            "    ],\n",
            "    \"token_logprobs\": [\n",
            "      -0.084499575,\n",
            "      -0.13960928,\n",
            "      -2.1977406,\n",
            "      -0.06784839,\n",
            "      -2.0699346,\n",
            "      -0.2904232,\n",
            "      -0.0246029,\n",
            "      -0.059372548\n",
            "    ],\n",
            "    \"tokens\": [\n",
            "      \"6\",\n",
            "      \"\\n\",\n",
            "      \"1\",\n",
            "      \"+\",\n",
            "      \"3\",\n",
            "      \"=\",\n",
            "      \"4\",\n",
            "      \"\\n\"\n",
            "    ],\n",
            "    \"top_logprobs\": [\n",
            "      {\n",
            "        \"1\": -5.257019,\n",
            "        \"2\": -3.9814146,\n",
            "        \"3\": -4.9558797,\n",
            "        \"5\": -3.8883283,\n",
            "        \"6\": -0.084499575\n",
            "      },\n",
            "      {\n",
            "        \"\\n\": -0.13960928,\n",
            "        \" \": -4.9482865,\n",
            "        \" (\": -4.020915,\n",
            "        \",\": -4.7713614,\n",
            "        \"\\u00a0\": -4.5964727\n",
            "      },\n",
            "      {\n",
            "        \"1\": -2.1977406,\n",
            "        \"2\": -1.9829503,\n",
            "        \"3\": -1.8792478,\n",
            "        \"4\": -2.4649906,\n",
            "        \"6\": -2.3231792\n",
            "      },\n",
            "      {\n",
            "        \"*\": -5.1001625,\n",
            "        \"+\": -0.06784839,\n",
            "        \"/\": -4.6147113,\n",
            "        \"x\": -4.2238383,\n",
            "        \"\\u00d7\": -4.9308624\n",
            "      },\n",
            "      {\n",
            "        \"1\": -1.4993131,\n",
            "        \"2\": -1.1626548,\n",
            "        \"3\": -2.0699346,\n",
            "        \"4\": -2.4263763,\n",
            "        \"6\": -2.3241692\n",
            "      },\n",
            "      {\n",
            "        \" =\": -5.5241,\n",
            "        \"+\": -1.4340427,\n",
            "        \"-\": -7.1435385,\n",
            "        \"=\": -0.2904232,\n",
            "        \"=-\": -7.2084374\n",
            "      },\n",
            "      {\n",
            "        \"1\": -6.030412,\n",
            "        \"2\": -6.1933684,\n",
            "        \"3\": -5.5968833,\n",
            "        \"4\": -0.0246029,\n",
            "        \"5\": -5.2526073\n",
            "      },\n",
            "      {\n",
            "        \"\\n\": -0.059372548,\n",
            "        \"\\n\\n\": -4.5789304,\n",
            "        \" \": -4.5420647,\n",
            "        \" (\": -5.1088147,\n",
            "        \",\": -5.5346303\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  \"text\": \"6\"\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF6DXqlay3tG"
      },
      "source": [
        "Holy cow that returned a lot of stuff! Let's start by looking at the tokens that GPT actually generated there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FQVKw4syHVb",
        "outputId": "e41de559-bf73-413e-85ce-875d4c06a2a5"
      },
      "source": [
        "r['choices'][0]['logprobs']['tokens']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['6', '\\n', '1', '+', '3', '=', '4', '\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2T4OPcAzFqd"
      },
      "source": [
        "Ah, so it generated '6' (what we saw in text) then '\\n' and thought that we'd ask it again what 1+3=.\n",
        "\n",
        "Why's it stopping? Well, we had the '\\n' as a stop token, meaning that once GPT found the stop token it'll stop and return the results so far. It goes a little past the stop token though so we ended up with 6 tokens instead of the 1 token that we cared about.\n",
        "\n",
        "Next, let's take a look at the tokens competing with '6' as the first token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSJPKHyqzEeR",
        "outputId": "3444c749-b1c2-4ddc-f1bf-506e7b25951b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject at 0x7f2404b6f888> JSON: {\n",
              "  \"1\": -5.257019,\n",
              "  \"2\": -3.9814146,\n",
              "  \"3\": -4.9558797,\n",
              "  \"5\": -3.8883283,\n",
              "  \"6\": -0.084499575\n",
              "}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9lQRbnBzkKQ"
      },
      "source": [
        "The logprobs are the probabiltiies that each token comes next. The top logprobs are the most likely next tokens; since we asked for 5 we got the 5 most likely tokens back in this field. We can convert the logprobs back to probabilites by taking e**token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChLXlrqsz8Vv"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDxGqXJizg3g",
        "outputId": "7ac75f9c-87e6-49a2-bb95-e4ff18be5cf7"
      },
      "source": [
        "rslts = r['choices'][0]['logprobs']['top_logprobs'][0]\n",
        "[(x, '{}%'.format(np.round(100*np.e**float(rslts[x]),2))) for x in rslts]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('1', '0.52%'), ('2', '1.87%'), ('3', '0.7%'), ('5', '2.05%'), ('6', '91.9%')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80-jAwpI0iWO"
      },
      "source": [
        "man it was pretty confident that '6' was next! But what about the 1+3=4 on the next line?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5TJC_GCz12T",
        "outputId": "5aeca719-df91-407e-c563-fcabfb07163c"
      },
      "source": [
        "rslts = r['choices'][0]['logprobs']['top_logprobs'][2]\n",
        "[(x, '{}%'.format(np.round(100*np.e**float(rslts[x]),2))) for x in rslts]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('1', '11.11%'),\n",
              " ('2', '13.77%'),\n",
              " ('3', '15.27%'),\n",
              " ('4', '8.5%'),\n",
              " ('6', '9.8%')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_QqnyI50suX"
      },
      "source": [
        "It wasn't as confident there; there was no reason to think it would be anything so it just kind of guessed off of whatever problems it had seen like this in this context. Wait, so it wasn't taking hte most probale token each time! I guess the default temperature isn't 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frSIDyBpJjxa",
        "outputId": "1ebd4fb5-2f68-40a1-b19d-8904b3cc7b70"
      },
      "source": [
        "r = openai.Completion.create(prompt=prompt, engine='davinci', stop=['\\n'], max_tokens=40, logprobs=5, temperature=0)\n",
        "print(r['choices'][0]['text'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7adgeK7JpjC",
        "outputId": "c4aa9eeb-22bb-4c6a-928f-265e64a26e74"
      },
      "source": [
        "r['choices'][0]['logprobs']['tokens']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['6', '\\n', '3', '+', '5', '=', '8', '\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkG5tsyhJmas",
        "outputId": "90c007b3-303a-4b82-b2f5-671f7cbf44d1"
      },
      "source": [
        "print(r['choices'][0]['text'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj2jIeVcIfKJ"
      },
      "source": [
        "Let's go take lower probability tokens using the temperature parameter. Temperature is a range of 0-2, were 2 is crazy non-probable next tokens. Normally temperature around .3 will result in some creativity, 1 a bit random, and above that nuts.\n",
        "\n",
        "So we'll re-request using temperature=1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORWrfd1J0riQ",
        "outputId": "89b96015-5d1e-40e0-8a84-44ffcf4911e5"
      },
      "source": [
        "r = openai.Completion.create(prompt=prompt, engine='davinci', stop=['\\n'], max_tokens=40, logprobs=5, temperature=1)\n",
        "print(r['choices'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"finish_reason\": \"stop\",\n",
            "  \"index\": 0,\n",
            "  \"logprobs\": {\n",
            "    \"text_offset\": [\n",
            "      10,\n",
            "      11,\n",
            "      11,\n",
            "      11,\n",
            "      11,\n",
            "      11,\n",
            "      11,\n",
            "      11\n",
            "    ],\n",
            "    \"token_logprobs\": [\n",
            "      -0.08648484,\n",
            "      -0.13950278,\n",
            "      -5.74998,\n",
            "      -4.896288,\n",
            "      -3.0244753,\n",
            "      -3.0125067,\n",
            "      -5.5629826,\n",
            "      -2.993689\n",
            "    ],\n",
            "    \"tokens\": [\n",
            "      \"6\",\n",
            "      \"\\n\",\n",
            "      \"So\",\n",
            "      \" for\",\n",
            "      \" any\",\n",
            "      \" given\",\n",
            "      \" base\",\n",
            "      \" and\"\n",
            "    ],\n",
            "    \"top_logprobs\": [\n",
            "      {\n",
            "        \"1\": -5.236021,\n",
            "        \"2\": -3.9320352,\n",
            "        \"3\": -4.9127364,\n",
            "        \"5\": -3.874693,\n",
            "        \"6\": -0.08648484\n",
            "      },\n",
            "      {\n",
            "        \"\\n\": -0.13950278,\n",
            "        \" \": -4.932246,\n",
            "        \" (\": -4.030875,\n",
            "        \",\": -4.7685347,\n",
            "        \"\\u00a0\": -4.527675\n",
            "      },\n",
            "      {\n",
            "        \"1\": -2.1801443,\n",
            "        \"2\": -1.978202,\n",
            "        \"3\": -1.8764564,\n",
            "        \"4\": -2.4651597,\n",
            "        \"6\": -2.3318436\n",
            "      },\n",
            "      {\n",
            "        \" 2\": -3.7843568,\n",
            "        \" far\": -3.6162126,\n",
            "        \" the\": -2.1928577,\n",
            "        \" we\": -2.984121,\n",
            "        \",\": -1.4871539\n",
            "      },\n",
            "      {\n",
            "        \" each\": -2.8782656,\n",
            "        \" every\": -2.5305257,\n",
            "        \" example\": -2.8920448,\n",
            "        \" the\": -1.6233945,\n",
            "        \" this\": -2.7263532\n",
            "      },\n",
            "      {\n",
            "        \" given\": -3.0125067,\n",
            "        \" integer\": -3.143679,\n",
            "        \" number\": -2.0074103,\n",
            "        \" positive\": -3.4560566,\n",
            "        \" two\": -2.6851676\n",
            "      },\n",
            "      {\n",
            "        \" integer\": -3.4783266,\n",
            "        \" number\": -1.7380811,\n",
            "        \" pair\": -3.8175545,\n",
            "        \" set\": -3.1046085,\n",
            "        \" value\": -3.4651089\n",
            "      },\n",
            "      {\n",
            "        \" \\\"\": -2.4538999,\n",
            "        \" and\": -2.993689,\n",
            "        \" b\": -3.0185816,\n",
            "        \" formula\": -2.759665,\n",
            "        \",\": -1.1658597\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  \"text\": \"6\"\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnCEeKoNI6y1"
      },
      "source": [
        "We can see the probabilities are still the same (GPT isn't quite deterministic)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM2TKjv9I4Mk",
        "outputId": "dac7e2ca-6e53-41de-d63d-3a02b92d45b8"
      },
      "source": [
        "rslts = r['choices'][0]['logprobs']['top_logprobs'][0]\n",
        "[(x, '{}%'.format(np.round(100*np.e**float(rslts[x]),2))) for x in rslts]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('1', '0.53%'),\n",
              " ('2', '1.96%'),\n",
              " ('3', '0.74%'),\n",
              " ('5', '2.08%'),\n",
              " ('6', '91.71%')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sG4xw93kJNTL",
        "outputId": "c64fee2a-cf31-4275-ebc4-d006a8345811"
      },
      "source": [
        "r['choices'][0]['logprobs']['tokens']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['6', '\\n', 'So', ' for', ' any', ' given', ' base', ' and']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6JKuv27J-Ww"
      },
      "source": [
        "woah, those weren't probable! They aren't in the top logprobs at all!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01WL7KIpJBTg",
        "outputId": "7779c6df-74af-4671-9338-de127a09c428"
      },
      "source": [
        "rslts = r['choices'][0]['logprobs']['top_logprobs'][2]\n",
        "[(x, '{}%'.format(np.round(100*np.e**float(rslts[x]),2))) for x in rslts]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('3', '15.31%'),\n",
              " ('2', '13.83%'),\n",
              " ('1', '11.3%'),\n",
              " ('6', '9.71%'),\n",
              " ('4', '8.5%')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "By1S32TMKBlO"
      },
      "source": [
        "So what were the probabilies of each of those? Wow, it just grabbed those low probability ones when it didn't know where to go lol"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8G0TzrhJFC4",
        "outputId": "97349249-7b63-407c-c2d5-1b4102620c8a"
      },
      "source": [
        "rslts = r['choices'][0]['logprobs']['token_logprobs']\n",
        "[(x, '{}%'.format(np.round(100*np.e**x),2)) for x in rslts]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(-0.08648484, '92.0%'),\n",
              " (-0.13950278, '87.0%'),\n",
              " (-5.74998, '0.0%'),\n",
              " (-4.896288, '1.0%'),\n",
              " (-3.0244753, '5.0%'),\n",
              " (-3.0125067, '5.0%'),\n",
              " (-5.5629826, '0.0%'),\n",
              " (-2.993689, '5.0%')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjiiaCF8Kpko"
      },
      "source": [
        "K, so if we want to try a bunch of different paths using temperature we can use the 'n' parameter to get more choices back so we don't need to use choices of 0 each time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ0BgxpGKR-m",
        "outputId": "85ef6d42-a81c-4e20-f1b3-b36bd78546be"
      },
      "source": [
        "r = openai.Completion.create(prompt=prompt, engine='davinci', stop=['\\n'], max_tokens=40, logprobs=5, n=4)\n",
        "[print(x['logprobs']['tokens']) for x in r['choices']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['3', '\\n', '2', '=', '0', '\\n', '2', '=']\n",
            "['6', '\\n', '4', '+', '4', '=', '8', '\\n']\n",
            "['6', '\\n', '5', '+', '1', '=', '6', '\\n']\n",
            "['6', '\\n', '6', '+', '5', '=', '11', '\\n']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None, None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH5OANJ0LDkN"
      },
      "source": [
        "cool! We managed to skip out of the 90% one in 1/4 lol\n",
        "\n",
        "We can also ask OpenAI to calculate which of those would have the highest average logprob and give just that back to us with the best_of parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPxG1hHVLJdi",
        "outputId": "89be41fd-d936-4abe-d33b-144674c11dbb"
      },
      "source": [
        "r = openai.Completion.create(prompt=prompt, engine='davinci', stop=['\\n'], max_tokens=40, logprobs=5, best_of=4)\n",
        "[print(x['logprobs']['tokens']) for x in r['choices']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['6', '\\n', '1', '+', '2', '+', '3', '+']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_AfmEGBLT8E"
      },
      "source": [
        "And we can request the n highest best_ofs (so return in order of highest -> lowest average logprob)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TePIsp9K2zZ",
        "outputId": "2165ce25-b37b-4ff5-fcf3-89d80e949f89"
      },
      "source": [
        "r = openai.Completion.create(prompt=prompt, engine='davinci', stop=['\\n'], max_tokens=40, logprobs=5, n=4, best_of=4)\n",
        "[print(x['logprobs']['tokens']) for x in r['choices']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['6', '\\n', '4', 'x', '4', '\\n', '4', 'x']\n",
            "['→', '6', '→', '1', '+', '4', '=', '5', '\\n', '4', '+', '6', '=', '→', '10', '→']\n",
            "['5', '\\n', '1', '+', '2', '+', '3', '+']\n",
            "['bytes:\\\\xce', 'bytes:\\\\xa6', '\\n', '0', '+', '2', '+', '4']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None, None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiA1sIM4Lf8B"
      },
      "source": [
        "but we can't ask for more than the top n best_of because it only generates best_of to order them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "hzaeH8vALW51",
        "outputId": "f6319edc-19a4-449d-fb04-adb55a32ffee"
      },
      "source": [
        "r = openai.Completion.create(prompt=prompt, engine='davinci', stop=['\\n'], max_tokens=40, logprobs=5, n=10, best_of=4)\n",
        "[print(x['logprobs']['tokens']) for x in r['choices']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidRequestError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-cfa9536d64ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompletion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'davinci'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogprobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_of\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logprobs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'choices'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, idempotency_key, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midempotency_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         response, _, api_key = requestor.request(\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         )\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, stream)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         )\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_api_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36minterpret_response\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    356\u001b[0m             )\n\u001b[1;32m    357\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpret_response_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minterpret_response_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36minterpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m             raise self.handle_error_response(\n\u001b[0;32m--> 378\u001b[0;31m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m             )\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidRequestError\u001b[0m: You requested that the server return more choices that it will generate (HINT: you must set 'n' (currently 10) to be at most 'best_of' (currently 4), or omit either parameter if you don't specifically want to use them.)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyOubydULsnG"
      },
      "source": [
        "Do we want to cover logit_bias / freq penalty etc here?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NOx091XLn9J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}